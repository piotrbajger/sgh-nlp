{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aviva Natural Language Processing SGH Talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Tweak matplotlib the figure sizes\n",
    "plt.rcParams['figure.figsize'] = [35, 25]\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data first. The 20 Newsgroups dataset is conveniently available through the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "news_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    ")\n",
    "\n",
    "news_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to \n",
    "\n",
    "1. Get top tokens from a Multinomial NB model.\n",
    "\n",
    "2. Compute macro-averaged metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(vect, clf, n=10, k=0):\n",
    "    \"\"\"\n",
    "    Lists top 'n' discriminant words in class.\n",
    "    \n",
    "    :param vect: Instance of a vectoriser used in feature extraction, e.g.\n",
    "        CountVectorizer, TfidfVectorizer, etc.\n",
    "    :param clf: Instance of a linear classifier used, e.g. MultinomialNB.\n",
    "    :param n: How many top features are to be printed.\n",
    "    :param k: Class index.\n",
    "\n",
    "    :return: A list of n most discriminant words in class.\n",
    "    \"\"\"\n",
    "    feature_names = vect.get_feature_names()\n",
    "    top_n = np.argsort(clf.coef_[k])[-n:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_n]\n",
    "\n",
    "    return top_features\n",
    "\n",
    "def compute_metrics(actual_y, pred_y):\n",
    "    \"\"\"\n",
    "    Returns macro metrics: precision, accuracy and F1-score.\n",
    "    \"\"\"\n",
    "    prec = metrics.precision_score(actual_y, pred_y, average='macro')\n",
    "    acc = metrics.accuracy_score(actual_y, pred_y)\n",
    "    f1 = metrics.f1_score(actual_y, pred_y, average='macro')\n",
    "    \n",
    "    return prec, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label: {news_train.target_names[news_train.target[3]]}\\n\\n\")\n",
    "print(news_train.data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the classes balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = {key: val for key, val in Counter(news_train.target).items()}\n",
    "train_counts = {key: train_counts[key] for key in sorted(train_counts, key=train_counts.get)}\n",
    "\n",
    "labels = [news_train.target_names[key] for key in train_counts.keys()]\n",
    "\n",
    "gig, ax =plt.subplots()\n",
    "ax.barh(range(len(train_counts)), list(train_counts.values()), 0.75, align='center')\n",
    "ax.set_yticks(np.arange(len(train_counts)))\n",
    "ax.set_yticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "train_x = vect.fit_transform(news_train.data)\n",
    "train_y = news_train.target\n",
    "\n",
    "test_x = vect.transform(news_test.data)\n",
    "test_y = news_test.target\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "print(\"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(f\"Training set: {len(news_train.target)}\")\n",
    "print(f\" Testing set: {len(news_test.target)}\")\n",
    "print(f\"Extracted {len(vect.get_feature_names())} features.\")\n",
    "\n",
    "# Define a classifier\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier\n",
    "nb.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_y = nb.predict(test_x)\n",
    "\n",
    "# Summary of the macro scores\n",
    "print(\"=\"*80)\n",
    "print(\"Scores:\")\n",
    "prec_1, acc_1, f1_1 = compute_metrics(test_y, pred_y)\n",
    "print(f\"Precision: {prec_1:.3f}\")\n",
    "print(f\" Accuracy: {acc_1:.3f}\")\n",
    "print(f\" F1-Score: {f1_1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "train_x = vect.fit_transform(news_train.data)\n",
    "train_y = news_train.target\n",
    "\n",
    "test_x = vect.transform(news_test.data)\n",
    "test_y = news_test.target\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "print(\"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(f\"Training set: {len(news_train.target)}\")\n",
    "print(f\" Testing set: {len(news_test.target)}\")\n",
    "print(f\"Extracted {len(vect.get_feature_names())} features.\")\n",
    "\n",
    "# Define a classifier\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier\n",
    "nb.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_y = nb.predict(test_x)\n",
    "\n",
    "# Summary of the macro scores\n",
    "print(\"=\"*80)\n",
    "print(\"Scores:\")\n",
    "prec_2, acc_2, f1_2 = compute_metrics(test_y, pred_y)\n",
    "print(f\"Precision: {prec_2:.3f} ({prec_1:.3f})\")\n",
    "print(f\" Accuracy: {acc_2:.3f} ({acc_1:.3f})\")\n",
    "print(f\" F1-Score: {f1_2:.3f} ({f1_1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top words per class\")\n",
    "for k, cat in enumerate(news_train.target_names):\n",
    "    print(f\"{cat:<25}\", get_top_features(vect, nb, n=10, k=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - TfidfVectorizer + stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words above are not very informative -- in NLP they're called *stopwords*. The nltk (Natural Language Processing Toolkig) has a list of all english stopwords. Removing them from the BoW document representation can improve accuracy and provide more informative \"top words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vectorizer -- this time with stopwords\n",
    "vect = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english')\n",
    ")\n",
    "\n",
    "train_x = vect.fit_transform(news_train.data)\n",
    "train_y = news_train.target\n",
    "\n",
    "test_x = vect.transform(news_test.data)\n",
    "test_y = news_test.target\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "print(\"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(f\"Training set: {len(news_train.target)}\")\n",
    "print(f\" Testing set: {len(news_test.target)}\")\n",
    "print(f\"Extracted {len(vect.get_feature_names())} features.\")\n",
    "\n",
    "# Define a classifier\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier\n",
    "nb.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_y = nb.predict(test_x)\n",
    "\n",
    "# Summary of the macro scores\n",
    "print(\"=\"*80)\n",
    "print(\"Scores:\")\n",
    "prec_3, acc_3, f1_3 = compute_metrics(test_y, pred_y)\n",
    "print(f\"Precision: {prec_3:.3f} ({prec_2:.3f})\")\n",
    "print(f\" Accuracy: {acc_3:.3f} ({acc_2:.3f})\")\n",
    "print(f\" F1-Score: {f1_3:.3f} ({f1_2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top words per class\")\n",
    "for k, cat in enumerate(news_train.target_names):\n",
    "    print(f\"{cat:<25}\", get_top_features(vect, nb, n=10, k=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a lot of noise: .edu, .com are of course parts of domains. Keith, Leveish are names. The model is actualy overfitting to posters' email addresses!\n",
    "\n",
    "So -- back to the drawing board..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - TfidfVectorizer + stopwords + cleaner data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label: {news_train.target_names[news_train.target[3]]}\\n\\n\")\n",
    "print(news_train.data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the cleaning has already been done for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "news_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers'),\n",
    ")\n",
    "news_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    remove=('headers', 'footers'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label: {news_train.target_names[news_train.target[3]]}\\n\\n\")\n",
    "print(news_train.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vectorizer\n",
    "vect = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'),\n",
    ")\n",
    "\n",
    "train_x = vect.fit_transform(news_train.data)\n",
    "train_y = news_train.target\n",
    "\n",
    "test_x = vect.transform(news_test.data)\n",
    "test_y = news_test.target\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "print(\"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(f\"Training set: {len(news_train.target)}\")\n",
    "print(f\" Testing set: {len(news_test.target)}\")\n",
    "print(f\"Extracted {len(vect.get_feature_names())} features.\")\n",
    "\n",
    "# Define a classifier\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier\n",
    "nb.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_y = nb.predict(test_x)\n",
    "\n",
    "# Summary of the macro scores\n",
    "print(\"=\"*80)\n",
    "print(\"Scores:\")\n",
    "prec_4, acc_4, f1_4 = compute_metrics(test_y, pred_y)\n",
    "print(f\"Precision: {prec_4:.3f} ({prec_3:.3f})\")\n",
    "print(f\" Accuracy: {acc_4:.3f} ({acc_3:.3f})\")\n",
    "print(f\" F1-Score: {f1_4:.3f} ({f1_3:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top words per class\")\n",
    "for k, cat in enumerate(news_train.target_names):\n",
    "    print(f\"{cat:<25}\", get_top_features(vect, nb, n=10, k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pprint_confusion_matrix(conf_matrix, labels):\n",
    "    df = pd.DataFrame(\n",
    "        data=conf_matrix,\n",
    "        index=labels,\n",
    "        columns=labels\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(conf_matrix)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation='vertical')\n",
    "    ax.set_yticklabels(labels)\n",
    "    \n",
    "    ax.set_xlabel(\"Predicted\", fontsize=30)\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.set_ylabel(\"True\", fontsize=30)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "pprint_confusion_matrix(metrics.confusion_matrix(test_y, pred_y), news_train.target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
